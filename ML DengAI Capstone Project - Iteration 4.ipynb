{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth iteration \n",
    "\n",
    "Trains models on data set of one city first and then train on data from the target city.\n",
    "\n",
    "No longer workin with Adaboost, only one submission tried with Convulational Neural Network, focusing mainly on LSTM and Random Forest, reasons for this are limited number of possible submissions (3 per day), previous attempts didn't get very good results.\n",
    "\n",
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error,make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create scorer\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "# Load Iquitos preprocessed Data \n",
    "features_train_iq = pd.read_csv(\"preprocessed data/dengue_features_train_iq.csv\")\n",
    "labels_train_iq = pd.read_csv(\"preprocessed data/dengue_labels_train_iq.csv\")\n",
    "features_test_iq = pd.read_csv(\"preprocessed data/dengue_features_test_iq.csv\")\n",
    "\n",
    "# drop all columns referencing times\n",
    "stripped_features_train_iq = features_train_iq.drop(['year','weekofyear','week_start_date'], axis=1)\n",
    "stripped_labels_train_iq = labels_train_iq.drop(['year','weekofyear'], axis=1)\n",
    "stripped_features_test_iq = features_test_iq.drop(['year','weekofyear','week_start_date'], axis=1)\n",
    "\n",
    "# Load San Juan preprocessed Data \n",
    "features_train_sj = pd.read_csv(\"preprocessed data/dengue_features_train_sj.csv\")\n",
    "labels_train_sj = pd.read_csv(\"preprocessed data/dengue_labels_train_sj.csv\")\n",
    "features_test_sj = pd.read_csv(\"preprocessed data/dengue_features_test_sj.csv\")\n",
    "\n",
    "stripped_features_train_sj = features_train_sj.drop(['year','weekofyear','week_start_date'], axis=1)\n",
    "stripped_labels_train_sj = labels_train_sj.drop(['year','weekofyear'], axis=1)\n",
    "stripped_features_test_sj = features_test_sj.drop(['year','weekofyear','week_start_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list or NumPy array.\n",
    "    n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "        indexes of removed rows\n",
    "\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def prepare_data_with_window (data_train, data_labels, data_test, window_size):\n",
    "    \n",
    "    data = data_train.append(data_test)\n",
    "    \n",
    "    data_w = series_to_supervised(data, n_in=window_size, dropnan=True)\n",
    "    \n",
    "    #split\n",
    "    data_train_w = data_w.iloc[ : (len(data_train) - window_size)]\n",
    "    data_test_w = data_w.iloc[(len(data_train) - window_size) : ]\n",
    "    data_labels_w = data_labels.iloc[window_size : ]\n",
    "    \n",
    "    return data_train_w, data_labels_w, data_test_w\n",
    " \n",
    "window_size = 20\n",
    "    \n",
    "# prepare IQ dataset with a window of size n\n",
    "w_stripped_features_train_iq, w_stripped_labels_train_iq, w_stripped_features_test_iq = prepare_data_with_window(\n",
    "    stripped_features_train_iq, \n",
    "    stripped_labels_train_iq, \n",
    "    stripped_features_test_iq, window_size)\n",
    "\n",
    "# prepare SJ dataset with a window of size n\n",
    "w_stripped_features_train_sj, w_stripped_labels_train_sj, w_stripped_features_test_sj = prepare_data_with_window(\n",
    "    stripped_features_train_sj, \n",
    "    stripped_labels_train_sj, \n",
    "    stripped_features_test_sj, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "reg = L1L2(l1=0.0, l2=0.00001)\n",
    "#reg = L1L2(l1=0.0, l2=0.00)\n",
    "\n",
    "columns_to_scale = w_stripped_features_test_iq.columns.difference(['data_set'])\n",
    "\n",
    "# Initialize a scaler and apply it to the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # default=(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iquitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be done on both, train and test features datasets, ensuring that the values in both datasets \n",
    "# remain of same magnitude. Therefore these datasets will be joined, applied the minmax normalization, and then splitted.\n",
    "w_stripped_features_train_iq['data_set'] = 'train'\n",
    "w_stripped_features_test_iq['data_set']  = 'test'\n",
    "\n",
    "dengue_norm_features_iq  = w_stripped_features_train_iq.append(w_stripped_features_test_iq)\n",
    "dengue_norm_features_iq[columns_to_scale] = scaler.fit_transform(dengue_norm_features_iq[columns_to_scale])\n",
    "\n",
    "# separate into the original datasets, dropping the temporary columns 'dataset'\n",
    "stripped_norm_dengue_features_train_iq = dengue_norm_features_iq[dengue_norm_features_iq['data_set'] == 'train']\n",
    "stripped_norm_dengue_features_train_iq = stripped_norm_dengue_features_train_iq.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_train_iq = stripped_norm_dengue_features_train_iq.drop(['data_set'], axis=1)\n",
    "\n",
    "stripped_norm_dengue_features_test_iq = dengue_norm_features_iq[dengue_norm_features_iq['data_set'] == 'test']\n",
    "stripped_norm_dengue_features_test_iq = stripped_norm_dengue_features_test_iq.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_test_iq = stripped_norm_dengue_features_test_iq.drop(['data_set'], axis=1)\n",
    "\n",
    "# normalize labels\n",
    "scalerLabels_iq = scaler.fit(w_stripped_labels_train_iq)\n",
    "stripped_norm_dengue_labels_train_iq = scalerLabels_iq.transform(w_stripped_labels_train_iq)\n",
    "\n",
    "# split data into train and test\n",
    "X_train_iq, X_test_iq = np.split(stripped_norm_dengue_features_train_iq, [int(.8*len(stripped_norm_dengue_features_train_iq))])\n",
    "y_train_iq, y_test_iq = np.split(stripped_norm_dengue_labels_train_iq, [int(.8*len(stripped_norm_dengue_labels_train_iq))])\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "X_submission_iq = stripped_norm_dengue_features_test_iq.values\n",
    "X_submission_iq_c = X_submission_iq.reshape((X_submission_iq.shape[0], X_submission_iq.shape[1], 1)) # conv1d\n",
    "X_submission_iq = X_submission_iq.reshape((X_submission_iq.shape[0], 1, X_submission_iq.shape[1])) # LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### San Juan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be done on both, train and test features datasets, ensuring that the values in both datasets \n",
    "# remain of same magnitude. Therefore these datasets will be joined, applied the minmax normalization, and then splitted.\n",
    "w_stripped_features_train_sj['data_set'] = 'train'\n",
    "w_stripped_features_test_sj['data_set']  = 'test'\n",
    "\n",
    "dengue_norm_features_sj  = w_stripped_features_train_sj.append(w_stripped_features_test_sj)\n",
    "dengue_norm_features_sj[columns_to_scale] = scaler.fit_transform(dengue_norm_features_sj[columns_to_scale])\n",
    "\n",
    "# separate into the original datasets, dropping the temporary columns 'dataset'\n",
    "stripped_norm_dengue_features_train_sj = dengue_norm_features_sj[dengue_norm_features_sj['data_set'] == 'train']\n",
    "stripped_norm_dengue_features_train_sj = stripped_norm_dengue_features_train_sj.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_train_sj = stripped_norm_dengue_features_train_sj.drop(['data_set'], axis=1)\n",
    "\n",
    "stripped_norm_dengue_features_test_sj = dengue_norm_features_sj[dengue_norm_features_sj['data_set'] == 'test']\n",
    "stripped_norm_dengue_features_test_sj = stripped_norm_dengue_features_test_sj.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_test_sj = stripped_norm_dengue_features_test_sj.drop(['data_set'], axis=1)\n",
    "\n",
    "# normalize labels\n",
    "scalerLabels_sj = scaler.fit(w_stripped_labels_train_sj)\n",
    "stripped_norm_dengue_labels_train_sj = scalerLabels_sj.transform(w_stripped_labels_train_sj)\n",
    "\n",
    "# split data into train and test\n",
    "X_train_sj, X_test_sj = np.split(stripped_norm_dengue_features_train_sj, [int(.8*len(stripped_norm_dengue_features_train_sj))])\n",
    "y_train_sj, y_test_sj = np.split(stripped_norm_dengue_labels_train_sj, [int(.8*len(stripped_norm_dengue_labels_train_sj))])\n",
    "\n",
    "# prepare test dataset\n",
    "X_submission_sj = stripped_norm_dengue_features_test_sj.values\n",
    "X_submission_sj_c = X_submission_sj.reshape((X_submission_sj.shape[0], X_submission_sj.shape[1], 1)) # conv1d\n",
    "X_submission_sj = X_submission_sj.reshape((X_submission_sj.shape[0], 1, X_submission_sj.shape[1])) # LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation datasets\n",
    "X_train  = X_train_iq.append(X_train_sj)\n",
    "X_test  = X_test_iq.append(X_test_sj)\n",
    "\n",
    "y_train = np.append(y_train_iq, y_train_sj, axis=0)\n",
    "y_test  = np.append(y_test_iq, y_test_sj, axis=0)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# reshape input for conv1D\n",
    "X_train_c = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_c = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# reshape data for LSTM, input to be 3D [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='relu', kernel_regularizer=reg))\n",
    "model.add(LSTM(300, return_sequences=True, activation='relu', kernel_regularizer=reg))\n",
    "model.add(LSTM(150, kernel_regularizer=reg))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: val_loss improved from inf to 0.00862, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00002: val_loss improved from 0.00862 to 0.00768, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00003: val_loss improved from 0.00768 to 0.00755, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00004: val_loss improved from 0.00755 to 0.00751, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00005: val_loss improved from 0.00751 to 0.00742, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00006: val_loss improved from 0.00742 to 0.00735, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00007: val_loss improved from 0.00735 to 0.00730, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00008: val_loss improved from 0.00730 to 0.00725, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00009: val_loss improved from 0.00725 to 0.00723, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00010: val_loss improved from 0.00723 to 0.00718, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00011: val_loss improved from 0.00718 to 0.00715, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00012: val_loss improved from 0.00715 to 0.00713, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00013: val_loss improved from 0.00713 to 0.00711, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00014: val_loss improved from 0.00711 to 0.00709, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00015: val_loss improved from 0.00709 to 0.00707, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00016: val_loss improved from 0.00707 to 0.00705, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00017: val_loss improved from 0.00705 to 0.00704, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00018: val_loss improved from 0.00704 to 0.00703, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00019: val_loss improved from 0.00703 to 0.00701, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00020: val_loss improved from 0.00701 to 0.00701, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00021: val_loss improved from 0.00701 to 0.00699, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00022: val_loss improved from 0.00699 to 0.00699, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00023: val_loss improved from 0.00699 to 0.00698, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 00025: val_loss improved from 0.00698 to 0.00697, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00026: val_loss improved from 0.00697 to 0.00696, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 00028: val_loss improved from 0.00696 to 0.00696, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 00030: val_loss improved from 0.00696 to 0.00695, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 00034: val_loss improved from 0.00695 to 0.00694, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 00037: val_loss improved from 0.00694 to 0.00693, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 00043: val_loss improved from 0.00693 to 0.00693, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 00047: val_loss improved from 0.00693 to 0.00693, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 00050: val_loss improved from 0.00693 to 0.00693, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00051: val_loss improved from 0.00693 to 0.00693, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 00054: val_loss improved from 0.00693 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 00056: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 00064: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 00066: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 00070: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 00075: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00076: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 00093: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 00105: val_loss improved from 0.00692 to 0.00692, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 00118: val_loss did not improve\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 00125: val_loss did not improve\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 00132: val_loss improved from 0.00692 to 0.00691, saving model to saved_models/weights.LSTM.it5.hdf5\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 00138: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00139: val_loss did not improve\n",
      "Epoch 00140: val_loss did not improve\n",
      "Epoch 00141: val_loss did not improve\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 00146: val_loss did not improve\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 00149: val_loss did not improve\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 00159: val_loss did not improve\n",
      "Epoch 00160: val_loss did not improve\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 00163: val_loss did not improve\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 00168: val_loss did not improve\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 00170: val_loss did not improve\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 00173: val_loss did not improve\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 00178: val_loss did not improve\n",
      "Epoch 00179: val_loss did not improve\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 00181: val_loss did not improve\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 00185: val_loss did not improve\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 00187: val_loss did not improve\n",
      "Epoch 00188: val_loss did not improve\n",
      "Epoch 00189: val_loss did not improve\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 00195: val_loss did not improve\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 00200: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.LSTM.it5.hdf5', verbose=2, \n",
    "                               save_best_only=True)\n",
    "\n",
    "# fit network       \n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), verbose=0, shuffle=False,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "# load best weights\n",
    "model.load_weights('saved_models/weights.LSTM.it5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for Iquitos\n",
    "y_submission = model.predict(X_submission_iq)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_iq = np.around(y_submission, decimals=0)\n",
    "y_submission_iq = y_submission_iq.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for San Juan\n",
    "y_submission = model.predict(X_submission_sj)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_sj = np.around(y_submission, decimals=0)\n",
    "y_submission_sj = y_submission_sj.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Juan - city,year,weekofyear,total_cases\n",
    "submission_sj = features_test_sj[['year','weekofyear']]\n",
    "submission_sj.insert( 0,'city','sj')\n",
    "\n",
    "df_y_submission_sj = pd.DataFrame(y_submission_sj, columns=['total_cases'])\n",
    "submission_sj = pd.concat([submission_sj, df_y_submission_sj], axis=1)\n",
    "\n",
    "# Iquitos - city,year,weekofyear,total_cases\n",
    "submission_iq = features_test_iq[['year','weekofyear']]\n",
    "submission_iq.insert( 0,'city','iq')\n",
    "\n",
    "df_y_submission_iq = pd.DataFrame(y_submission_iq, columns=['total_cases'])\n",
    "submission_iq = pd.concat([submission_iq, df_y_submission_iq], axis=1)\n",
    "\n",
    "# join both predictions\n",
    "submission = pd.concat([submission_sj, submission_iq])\n",
    "submission = submission.reset_index(drop = True)\n",
    "\n",
    "#write into csv\n",
    "submission.to_csv(\"Submission/Submission_it5_lstm_b16_L2_w20.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Submission Score \n",
    "\n",
    "#### With kernel regularizer L2 (0.00001),  Batch Size 8 and Window Size 8:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 16 and Window Size4:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without kernel regularizer,  Batch Size 32 and window size 8:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 32 and Window Size 8:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 32 and Window Size 10:\n",
    "Bad result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 32 and Window Size 12:\n",
    "Bad result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification with 1D convolutions\n",
    "\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "model_c = Sequential()\n",
    "model_c.add(Conv1D(96, 2, activation='relu', input_shape=( X_train_c.shape[1], 1), kernel_regularizer=reg))\n",
    "model_c.add(Conv1D(96, 2, activation='relu', kernel_regularizer=reg))\n",
    "model_c.add(MaxPooling1D(1))\n",
    "model_c.add(Conv1D(192, 2, activation='relu', kernel_regularizer=reg))\n",
    "model_c.add(Conv1D(192, 2, activation='relu', kernel_regularizer=reg))\n",
    "model_c.add(GlobalAveragePooling1D())\n",
    "model_c.add(Dropout(0.5))\n",
    "model_c.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_c.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: val_loss improved from inf to 0.01363, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00002: val_loss improved from 0.01363 to 0.01297, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00003: val_loss improved from 0.01297 to 0.01251, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00004: val_loss improved from 0.01251 to 0.01007, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 00006: val_loss improved from 0.01007 to 0.00967, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss improved from 0.00967 to 0.00793, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss improved from 0.00793 to 0.00783, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss improved from 0.00783 to 0.00763, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss improved from 0.00763 to 0.00744, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss improved from 0.00744 to 0.00736, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss improved from 0.00736 to 0.00718, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 00023: val_loss improved from 0.00718 to 0.00709, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 00025: val_loss improved from 0.00709 to 0.00708, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 00028: val_loss improved from 0.00708 to 0.00703, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00029: val_loss improved from 0.00703 to 0.00703, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 00039: val_loss improved from 0.00703 to 0.00699, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 00055: val_loss improved from 0.00699 to 0.00698, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 00060: val_loss improved from 0.00698 to 0.00697, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 00064: val_loss improved from 0.00697 to 0.00697, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 00066: val_loss improved from 0.00697 to 0.00696, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 00071: val_loss improved from 0.00696 to 0.00694, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 00073: val_loss improved from 0.00694 to 0.00693, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 00088: val_loss improved from 0.00693 to 0.00690, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 00093: val_loss improved from 0.00690 to 0.00689, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 00095: val_loss improved from 0.00689 to 0.00688, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00096: val_loss improved from 0.00688 to 0.00687, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 00107: val_loss improved from 0.00687 to 0.00685, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 00113: val_loss improved from 0.00685 to 0.00685, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00114: val_loss improved from 0.00685 to 0.00684, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 00118: val_loss improved from 0.00684 to 0.00684, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 00125: val_loss did not improve\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 00128: val_loss improved from 0.00684 to 0.00682, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 00134: val_loss improved from 0.00682 to 0.00681, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 00138: val_loss did not improve\n",
      "Epoch 00139: val_loss improved from 0.00681 to 0.00681, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00140: val_loss did not improve\n",
      "Epoch 00141: val_loss did not improve\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 00146: val_loss improved from 0.00681 to 0.00678, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 00149: val_loss did not improve\n",
      "Epoch 00150: val_loss improved from 0.00678 to 0.00676, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 00155: val_loss improved from 0.00676 to 0.00676, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 00159: val_loss did not improve\n",
      "Epoch 00160: val_loss did not improve\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 00163: val_loss improved from 0.00676 to 0.00676, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 00168: val_loss improved from 0.00676 to 0.00675, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 00170: val_loss improved from 0.00675 to 0.00674, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 00173: val_loss did not improve\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 00178: val_loss did not improve\n",
      "Epoch 00179: val_loss did not improve\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 00181: val_loss improved from 0.00674 to 0.00674, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 00185: val_loss improved from 0.00674 to 0.00673, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 00187: val_loss did not improve\n",
      "Epoch 00188: val_loss did not improve\n",
      "Epoch 00189: val_loss improved from 0.00673 to 0.00672, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 00195: val_loss improved from 0.00672 to 0.00668, saving model to saved_models/weights.conv1D.b32.it2.iq.hdf5\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 00200: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.conv1D.b32.it2.iq.hdf5', verbose=2, \n",
    "                               save_best_only=True)\n",
    "        \n",
    "history = model_c.fit(X_train_c, y_train, epochs=200, batch_size=32, validation_data=(X_test_c, y_test), verbose=0, shuffle=False,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "# load best weights\n",
    "model_c.load_weights('saved_models/weights.conv1D.b32.it2.iq.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_submission = model_c.predict(X_submission_iq_c)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_iq = np.around(y_submission, decimals=0)\n",
    "y_submission_iq = y_submission_iq.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_submission = model_c.predict(X_submission_sj_c)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_sj = np.around(y_submission, decimals=0)\n",
    "y_submission_sj = y_submission_sj.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Conv1 Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Juan - city,year,weekofyear,total_cases\n",
    "submission_sj = features_test_sj[['year','weekofyear']]\n",
    "submission_sj.insert( 0,'city','sj')\n",
    "\n",
    "df_y_submission_sj = pd.DataFrame(y_submission_sj, columns=['total_cases'])\n",
    "submission_sj = pd.concat([submission_sj, df_y_submission_sj], axis=1)\n",
    "\n",
    "# Iquitos - city,year,weekofyear,total_cases\n",
    "submission_iq = features_test_iq[['year','weekofyear']]\n",
    "submission_iq.insert( 0,'city','iq')\n",
    "\n",
    "df_y_submission_iq = pd.DataFrame(y_submission_iq, columns=['total_cases'])\n",
    "submission_iq = pd.concat([submission_iq, df_y_submission_iq], axis=1)\n",
    "\n",
    "# join both predictions\n",
    "submission = pd.concat([submission_sj, submission_iq])\n",
    "submission = submission.reset_index(drop = True)\n",
    "\n",
    "#write into csv\n",
    "submission.to_csv(\"Submission/Submission_5_conv1d_b32_L2_w12.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001), Batch Size 32, window 12:\n",
    "35.2380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001), Batch Size 32, window 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
