{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth iteration \n",
    "\n",
    "Trains models on data set of other town first and then train on data set of the target town\n",
    "\n",
    "No longer workin with Adaboost, only one submission tried with Convulational Neural Network, focusing mainly on LSTM and Random Forest, reasons for this are limited number of possible submissions (3 per day), previous attempts didn't get very good results.\n",
    "\n",
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error,make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create scorer\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "# Load Iquitos preprocessed Data \n",
    "features_train_iq = pd.read_csv(\"preprocessed data/dengue_features_train_iq.csv\")\n",
    "labels_train_iq = pd.read_csv(\"preprocessed data/dengue_labels_train_iq.csv\")\n",
    "features_test_iq = pd.read_csv(\"preprocessed data/dengue_features_test_iq.csv\")\n",
    "\n",
    "# drop all columns referencing times\n",
    "stripped_features_train_iq = features_train_iq.drop(['year','weekofyear','week_start_date'], axis=1)\n",
    "stripped_labels_train_iq = labels_train_iq.drop(['year','weekofyear'], axis=1)\n",
    "stripped_features_test_iq = features_test_iq.drop(['year','weekofyear','week_start_date'], axis=1)\n",
    "\n",
    "# Load San Juan preprocessed Data \n",
    "features_train_sj = pd.read_csv(\"preprocessed data/dengue_features_train_sj.csv\")\n",
    "labels_train_sj = pd.read_csv(\"preprocessed data/dengue_labels_train_sj.csv\")\n",
    "features_test_sj = pd.read_csv(\"preprocessed data/dengue_features_test_sj.csv\")\n",
    "\n",
    "stripped_features_train_sj = features_train_sj.drop(['year','weekofyear','week_start_date'], axis=1)\n",
    "stripped_labels_train_sj = labels_train_sj.drop(['year','weekofyear'], axis=1)\n",
    "stripped_features_test_sj = features_test_sj.drop(['year','weekofyear','week_start_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list or NumPy array.\n",
    "    n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "        indexes of removed rows\n",
    "\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def prepare_data_with_window (data_train, data_labels, data_test, window_size):\n",
    "    \n",
    "    data = data_train.append(data_test)\n",
    "    \n",
    "    data_w = series_to_supervised(data, n_in=window_size, dropnan=True)\n",
    "    \n",
    "    #split\n",
    "    data_train_w = data_w.iloc[ : (len(data_train) - window_size)]\n",
    "    data_test_w = data_w.iloc[(len(data_train) - window_size) : ]\n",
    "    data_labels_w = data_labels.iloc[window_size : ]\n",
    "    \n",
    "    return data_train_w, data_labels_w, data_test_w\n",
    " \n",
    "window_size = 40\n",
    "    \n",
    "# prepare IQ dataset with a window of size 2\n",
    "w_stripped_features_train_iq, w_stripped_labels_train_iq, w_stripped_features_test_iq = prepare_data_with_window(\n",
    "    stripped_features_train_iq, \n",
    "    stripped_labels_train_iq, \n",
    "    stripped_features_test_iq, window_size)\n",
    "\n",
    "# prepare SJ dataset with a window of size 2\n",
    "w_stripped_features_train_sj, w_stripped_labels_train_sj, w_stripped_features_test_sj = prepare_data_with_window(\n",
    "    stripped_features_train_sj, \n",
    "    stripped_labels_train_sj, \n",
    "    stripped_features_test_sj, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "reg = L1L2(l1=0.0, l2=0.00001)\n",
    "#reg = L1L2(l1=0.0, l2=0.00)\n",
    "\n",
    "columns_to_scale = w_stripped_features_test_iq.columns.difference(['data_set'])\n",
    "\n",
    "# Initialize a scaler and apply it to the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # default=(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iquitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be done on both, train and test features datasets, ensuring that the values in both datasets \n",
    "# remain of same magnitude. Therefore these datasets will be joined, applied the minmax normalization, and then splitted.\n",
    "w_stripped_features_train_iq['data_set'] = 'train'\n",
    "w_stripped_features_test_iq['data_set']  = 'test'\n",
    "\n",
    "dengue_norm_features_iq  = w_stripped_features_train_iq.append(w_stripped_features_test_iq)\n",
    "dengue_norm_features_iq[columns_to_scale] = scaler.fit_transform(dengue_norm_features_iq[columns_to_scale])\n",
    "\n",
    "# separate into the original datasets, dropping the temporary columns 'dataset'\n",
    "stripped_norm_dengue_features_train_iq = dengue_norm_features_iq[dengue_norm_features_iq['data_set'] == 'train']\n",
    "stripped_norm_dengue_features_train_iq = stripped_norm_dengue_features_train_iq.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_train_iq = stripped_norm_dengue_features_train_iq.drop(['data_set'], axis=1)\n",
    "\n",
    "stripped_norm_dengue_features_test_iq = dengue_norm_features_iq[dengue_norm_features_iq['data_set'] == 'test']\n",
    "stripped_norm_dengue_features_test_iq = stripped_norm_dengue_features_test_iq.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_test_iq = stripped_norm_dengue_features_test_iq.drop(['data_set'], axis=1)\n",
    "\n",
    "# normalize labels\n",
    "scalerLabels_iq = scaler.fit(w_stripped_labels_train_iq)\n",
    "stripped_norm_dengue_labels_train_iq = scalerLabels_iq.transform(w_stripped_labels_train_iq)\n",
    "\n",
    "# split data into train and test\n",
    "X_train_iq, X_test_iq = np.split(stripped_norm_dengue_features_train_iq, [int(.8*len(stripped_norm_dengue_features_train_iq))])\n",
    "y_train_iq, y_test_iq = np.split(stripped_norm_dengue_labels_train_iq, [int(.8*len(stripped_norm_dengue_labels_train_iq))])\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "X_submission_iq = stripped_norm_dengue_features_test_iq.values\n",
    "X_submission_iq_c = X_submission_iq.reshape((X_submission_iq.shape[0], X_submission_iq.shape[1], 1)) # conv1d\n",
    "X_submission_iq = X_submission_iq.reshape((X_submission_iq.shape[0], 1, X_submission_iq.shape[1])) # LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### San Juan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization should be done on both, train and test features datasets, ensuring that the values in both datasets \n",
    "# remain of same magnitude. Therefore these datasets will be joined, applied the minmax normalization, and then splitted.\n",
    "w_stripped_features_train_sj['data_set'] = 'train'\n",
    "w_stripped_features_test_sj['data_set']  = 'test'\n",
    "\n",
    "dengue_norm_features_sj  = w_stripped_features_train_sj.append(w_stripped_features_test_sj)\n",
    "dengue_norm_features_sj[columns_to_scale] = scaler.fit_transform(dengue_norm_features_sj[columns_to_scale])\n",
    "\n",
    "# separate into the original datasets, dropping the temporary columns 'dataset'\n",
    "stripped_norm_dengue_features_train_sj = dengue_norm_features_sj[dengue_norm_features_sj['data_set'] == 'train']\n",
    "stripped_norm_dengue_features_train_sj = stripped_norm_dengue_features_train_sj.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_train_sj = stripped_norm_dengue_features_train_sj.drop(['data_set'], axis=1)\n",
    "\n",
    "stripped_norm_dengue_features_test_sj = dengue_norm_features_sj[dengue_norm_features_sj['data_set'] == 'test']\n",
    "stripped_norm_dengue_features_test_sj = stripped_norm_dengue_features_test_sj.reset_index(drop = True)\n",
    "stripped_norm_dengue_features_test_sj = stripped_norm_dengue_features_test_sj.drop(['data_set'], axis=1)\n",
    "\n",
    "# normalize labels\n",
    "scalerLabels_sj = scaler.fit(w_stripped_labels_train_sj)\n",
    "stripped_norm_dengue_labels_train_sj = scalerLabels_sj.transform(w_stripped_labels_train_sj)\n",
    "\n",
    "# split data into train and test\n",
    "X_train_sj, X_test_sj = np.split(stripped_norm_dengue_features_train_sj, [int(.8*len(stripped_norm_dengue_features_train_sj))])\n",
    "y_train_sj, y_test_sj = np.split(stripped_norm_dengue_labels_train_sj, [int(.8*len(stripped_norm_dengue_labels_train_sj))])\n",
    "\n",
    "# prepare test dataset\n",
    "X_submission_sj = stripped_norm_dengue_features_test_sj.values\n",
    "X_submission_sj_c = X_submission_sj.reshape((X_submission_sj.shape[0], X_submission_sj.shape[1], 1)) # conv1d\n",
    "X_submission_sj = X_submission_sj.reshape((X_submission_sj.shape[0], 1, X_submission_sj.shape[1])) # LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation IQ datasets\n",
    "X_train_sj_m  = X_train_iq.append(X_train_sj)\n",
    "X_test_sj_m  = X_test_iq.append(X_test_sj)\n",
    "\n",
    "y_train_sj_m = np.append(y_train_iq, y_train_sj, axis=0)\n",
    "y_test_sj_m  = np.append(y_test_iq, y_test_sj, axis=0)\n",
    "\n",
    "X_train_sj_m = X_train_sj_m.values\n",
    "X_test_sj_m = X_test_sj_m.values\n",
    "\n",
    "# reshape input for conv1D\n",
    "X_train_sj_c = X_train_sj_m.reshape((X_train_sj_m.shape[0], X_train_sj_m.shape[1], 1))\n",
    "X_test_sj_c = X_test_sj_m.reshape((X_test_sj_m.shape[0], X_test_sj_m.shape[1], 1))\n",
    "\n",
    "# reshape data for LSTM, input to be 3D [samples, timesteps, features]\n",
    "X_train_sj_m = X_train_sj_m.reshape((X_train_sj_m.shape[0], 1, X_train_sj_m.shape[1]))\n",
    "X_test_sj_m = X_test_sj_m.reshape((X_test_sj_m.shape[0], 1, X_test_sj_m.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sj_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation SJ datasets\n",
    "X_train_iq_m  = X_train_sj.append(X_train_iq)\n",
    "X_test_iq_m  = X_test_sj.append(X_test_iq)\n",
    "\n",
    "y_train_iq_m = np.append(y_train_sj, y_train_iq, axis=0)\n",
    "y_test_iq_m  = np.append(y_test_sj, y_test_iq, axis=0)\n",
    "\n",
    "X_train_iq_m = X_train_iq_m.values\n",
    "X_test_iq_m = X_test_iq_m.values\n",
    "\n",
    "\n",
    "# reshape input for conv1D\n",
    "X_train_iq_c = X_train_iq_m.reshape((X_train_iq_m.shape[0], X_train_iq_m.shape[1], 1))\n",
    "X_test_iq_c = X_test_iq_m.reshape((X_test_iq_m.shape[0], X_test_iq_m.shape[1], 1))\n",
    "\n",
    "# reshape data for LSTM, input to be 3D [samples, timesteps, features]\n",
    "X_train_iq_m = X_train_iq_m.reshape((X_train_iq_m.shape[0], 1, X_train_iq_m.shape[1]))\n",
    "X_test_iq_m = X_test_iq_m.reshape((X_test_iq_m.shape[0], 1, X_test_iq_m.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design network\n",
    "model_iq = Sequential()\n",
    "model_iq.add(LSTM(150, input_shape=(X_train_iq_m.shape[1], X_train_iq_m.shape[2]), return_sequences=True, activation='relu', kernel_regularizer=reg))\n",
    "model_iq.add(LSTM(300, return_sequences=True, activation='relu', kernel_regularizer=reg))\n",
    "model_iq.add(LSTM(150, kernel_regularizer=reg))\n",
    "model_iq.add(Dropout(0.2))\n",
    "model_iq.add(Dense(1))\n",
    "model_iq.add(Activation(\"linear\"))\n",
    "model_iq.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design network\n",
    "model_sj = Sequential()\n",
    "model_sj.add(LSTM(150, input_shape=(X_train_sj_m.shape[1], X_train_sj_m.shape[2]), return_sequences=True, activation='relu', kernel_regularizer=reg))\n",
    "model_sj.add(LSTM(300, return_sequences=True, activation='relu', kernel_regularizer=reg))\n",
    "model_sj.add(LSTM(150, kernel_regularizer=reg))\n",
    "model_sj.add(Dropout(0.2))\n",
    "model_sj.add(Dense(1))\n",
    "model_sj.add(Activation(\"linear\"))\n",
    "model_sj.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sj.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "checkpointer_sj = ModelCheckpoint(filepath='saved_models/weights.LSTM.it5.sj.001.hdf5', verbose=2, \n",
    "                               save_best_only=True)\n",
    "\n",
    "# fit network       \n",
    "history = model_sj.fit(X_train_sj_m, y_train_sj_m, epochs=200, batch_size=40, validation_data=(X_test_sj_m, y_test_sj_m), verbose=0, shuffle=False,\n",
    "                    callbacks=[checkpointer_sj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "model_sj.load_weights('saved_models/weights.LSTM.it5.sj.001.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "checkpointer_iq = ModelCheckpoint(filepath='saved_models/weights.LSTM.it5.iq.001.hdf5', verbose=2, \n",
    "                               save_best_only=True)\n",
    "\n",
    "# fit network       \n",
    "history = model_iq.fit(X_train_iq_m, y_train_iq_m, epochs=200, batch_size=40, validation_data=(X_test_iq_m, y_test_iq_m), verbose=0, shuffle=False,\n",
    "                    callbacks=[checkpointer_iq])\n",
    "\n",
    "# load best weights\n",
    "model_iq.load_weights('saved_models/weights.LSTM.it5.iq.001.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for Iquitos\n",
    "y_submission = model_iq.predict(X_submission_iq)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_iq = np.around(y_submission, decimals=0)\n",
    "y_submission_iq = y_submission_iq.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for San Juan\n",
    "y_submission = model_sj.predict(X_submission_sj)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_sj = np.around(y_submission, decimals=0)\n",
    "y_submission_sj = y_submission_sj.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Juan - city,year,weekofyear,total_cases\n",
    "submission_sj = features_test_sj[['year','weekofyear']]\n",
    "submission_sj.insert( 0,'city','sj')\n",
    "\n",
    "df_y_submission_sj = pd.DataFrame(y_submission_sj, columns=['total_cases'])\n",
    "submission_sj = pd.concat([submission_sj, df_y_submission_sj], axis=1)\n",
    "\n",
    "# Iquitos - city,year,weekofyear,total_cases\n",
    "submission_iq = features_test_iq[['year','weekofyear']]\n",
    "submission_iq.insert( 0,'city','iq')\n",
    "\n",
    "df_y_submission_iq = pd.DataFrame(y_submission_iq, columns=['total_cases'])\n",
    "submission_iq = pd.concat([submission_iq, df_y_submission_iq], axis=1)\n",
    "\n",
    "# join both predictions\n",
    "submission = pd.concat([submission_sj, submission_iq])\n",
    "submission = submission.reset_index(drop = True)\n",
    "\n",
    "#write into csv\n",
    "submission.to_csv(\"Submission/Submission_it5_lstm_b40_L2_w44.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Submission Score \n",
    "\n",
    "#### With kernel regularizer L2 (0.00001),  Batch Size 8 and Window Size 8:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 16 and Window Size4:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without kernel regularizer,  Batch Size 32 and window size 8:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 32 and Window Size 8:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 32 and Window Size 10:\n",
    "mau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001),  Batch Size 32 and Window Size 12:\n",
    "\n",
    "MAU RESULTADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification with 1D convolutions\n",
    "\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "model_c = Sequential()\n",
    "model_c.add(Conv1D(96, 2, activation='relu', input_shape=( X_train_c.shape[1], 1), kernel_regularizer=reg))\n",
    "model_c.add(Conv1D(96, 2, activation='relu', kernel_regularizer=reg))\n",
    "model_c.add(MaxPooling1D(1))\n",
    "model_c.add(Conv1D(192, 2, activation='relu', kernel_regularizer=reg))\n",
    "model_c.add(Conv1D(192, 2, activation='relu', kernel_regularizer=reg))\n",
    "model_c.add(GlobalAveragePooling1D())\n",
    "model_c.add(Dropout(0.5))\n",
    "model_c.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_c.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.conv1D.b32.it2.iq.hdf5', verbose=2, \n",
    "                               save_best_only=True)\n",
    "        \n",
    "history = model_c.fit(X_train_c, y_train, epochs=200, batch_size=32, validation_data=(X_test_c, y_test), verbose=0, shuffle=False,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "# load best weights\n",
    "model_c.load_weights('saved_models/weights.conv1D.b32.it2.iq.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_submission = model_c.predict(X_submission_iq_c)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_iq = np.around(y_submission, decimals=0)\n",
    "y_submission_iq = y_submission_iq.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_submission = model_c.predict(X_submission_sj_c)\n",
    "\n",
    "# invert scaling for forecast\n",
    "y_submission = scalerLabels_iq.inverse_transform(y_submission)\n",
    "y_submission_sj = np.around(y_submission, decimals=0)\n",
    "y_submission_sj = y_submission_sj.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Conv1 Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Juan - city,year,weekofyear,total_cases\n",
    "submission_sj = features_test_sj[['year','weekofyear']]\n",
    "submission_sj.insert( 0,'city','sj')\n",
    "\n",
    "df_y_submission_sj = pd.DataFrame(y_submission_sj, columns=['total_cases'])\n",
    "submission_sj = pd.concat([submission_sj, df_y_submission_sj], axis=1)\n",
    "\n",
    "# Iquitos - city,year,weekofyear,total_cases\n",
    "submission_iq = features_test_iq[['year','weekofyear']]\n",
    "submission_iq.insert( 0,'city','iq')\n",
    "\n",
    "df_y_submission_iq = pd.DataFrame(y_submission_iq, columns=['total_cases'])\n",
    "submission_iq = pd.concat([submission_iq, df_y_submission_iq], axis=1)\n",
    "\n",
    "# join both predictions\n",
    "submission = pd.concat([submission_sj, submission_iq])\n",
    "submission = submission.reset_index(drop = True)\n",
    "\n",
    "#write into csv\n",
    "submission.to_csv(\"Submission/Submission_5_conv1d_b32_L2_w12.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001), Batch Size 32, window 12:\n",
    "35.2380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With kernel regularizer L2 (0.00001), Batch Size 32, window 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
